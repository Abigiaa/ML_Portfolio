---
title: "SVM Regression"
author: "Aditi Chaudhari and Abigail Solomon"
output:
  pdf_document: default
  html_notebook: default
---

##SVM Regression
Generally, in Regression technique,patterns in the sample data are identified by understanding the numbers-their values and correlations, then it reproduces the predictions of continuous outcomes.Now in our case, we will use the SVM regression  to predict the price of diamond based on attributes of the item as indicators.


##Load necessary libraries
```{r}
library(e1071)
library(ggplot2)
library(caret, warn.conflicts = FALSE)
```


##Import the Data set

###Source of the Data Set

Gem stone price prediction data set: ['diamond'dataset](https://www.kaggle.com/datasets/colearninglounge/gemstone-price-prediction)

###Read data
We use the read.csv() function to read a csv. Using the dim() structure function, We will see that the data set contain prices and other attributes of the diamond, and it has 26967 observations of 11 variables along with their names.
```{r}

#Read data
df <- read.csv("diamonds.csv")
dim(df)
names(df)

```

##Data Preprocessing
Data processing is converting train raw data sets into meaningful sets that are usable.We will be examining the specific types and steps of data cleaning, changing some variables to factors and later the scaling before analyzing the data. 

###Data cleaning
In order to make our data set for machine learning more meaningful, we may need to fix or remove missing or unwanted, data instances which may not help to solve the problem, from the data set. The sapply() function gives the number of missing values in each column, in this case the depth has 697 NA's.The unique functions retrieves that there are no repetitions to delete. 
```{r}

sapply(df, function(x) sum(is.na(x)==TRUE))
df <- unique(df)
dim(df)


```

###Data Sampling
Large data sets  consume a lot of memory and took ages to run, in our case we are going to take only the first 20,000 observations.We will remove the 697 NA's and the first column, and the rows after the 20,000th. and get the new dim of the new data set
```{r}
df$X <- NULL
df <- na.omit(df)
dim(df)

```
```{r}
df <- df[-c(20001:26270), ]
str(df)

```

##Data Split
We are going to split our data set into training, validation and tests sets. 60% of our data will be attributed to the train data set, 20% will be attributed to the validation data, and the rest 20% will be attributed to the test data. We will then find the dimensions of the data frame using the dim() function.

```{r}

set.seed(1234)
groups <- c(train=.6, test=.2, validate=.2)
i <- sample(cut(1:nrow(df),
nrow(df)*cumsum(c(0,groups)), labels=names(groups)))
train <- df[i=="train",]
test <- df[i=="test",]
vald <- df[i=="validate",]
dim(df)

```
### Converting to factors

```{r}
df$cut <- factor(df$cut)
df$color <- as.factor(df$color)
df$clarity <- factor(df$clarity) 

```

##Data Exploration
Data exploration helps us to gain insight into the raw train data and findings of R built-in functions.We will print the first and last six rows using the head() and tail() functions respectively.The summary () function applied on the Amount vector, calculates summary statistics for each of them, it prints the Minimum value, the 1st quartile's value (25th percentile), the median value, the 3rd quartile's value (75th percentile) and the maximum value. 

###The first six observations
```{r}

head(train)

```

###The last six observations
```{r}

tail(train)

```

###Summary of the train data set
```{r}

summary(train)

```

##Visual Data Exploration
Data visualization present train data contents in graphical or picture format, enables us to grasp and understand analytics in an easier manner and be able to communicate what has been learned about the data to others, it is also optically entertaining.

###Plot 
The plot graph shows that there is a linear relationship between price and carat, both increase linearly, of course there are some out liers.

```{r, warning=FALSE}

par(mfrow=c(1,2))
plot(train$price, train$carat,
     xlab="Price", ylab="Carat")

```

###Histogram
The Histogram graph displays the frequency of the x values, in our case it the percentage from the table function. the geom_vline can indicate our mean dashed line.

```{r}
ggplot(train, aes(x=table)) + 
  geom_histogram(binwidth = 1, color = "black", fill = "cadetblue") + 
  geom_vline(aes(xintercept=mean(table)), color = "coral4", linetype = "dashed") +
  labs(title = "                     Table Percentage", x = "Percentage", y = "Frequency")

```

###Barplot
Let us see the levels of the cut,color and clarity in bar plot. The ideal cut,SI1 and VS2 clarity have high frequency.

```{r}

barplot(table(train$cut), 
        main="Levels of Cut Attribute", xlab="Cut", ylab="Count")
```
```{r}
barplot(table(train$color), 
        main="Levels of Color Attribute", xlab="Color", ylab="Count")

```



```{r}

barplot(table(train$clarity), 
        main="Levels of Clarity Attribute", xlab="Clarity", ylab="Count")

```

##Data Modeling
We are going to build the linear regression, linear SVM, the Polynomial SVM,and the Radial Kernels SVM and see the accuracy of each model.

###Linear Regression model
Summary of the model:
```{r , warning=FALSE}
lm1 <- lm(price~carat+cut+color+clarity, data=train)
summary(lm1)

```
###Plot Residuals

```{r}

par(mfrow = c(2,2))
plot(lm1)

```


### Evaluate on the test set
The model has correlation of 95%, which is nice, and mse value 1247793

```{r}

pred_lm <- predict(lm1, newdata=test)
cor_lm <- cor(pred_lm, test$price)
mse_lm <- mean((pred_lm-test$price)^2) 
rmse_lm <- sqrt(mse_lm)
print(paste('correlation:', cor_lm))
print(paste('mse:', mse_lm))
print(paste('rmse:', rmse_lm))

```

###Linear SVM model
Let us build an SVM1 model on the train set using cost=10 and kernel="linear".

```{r}

svm1 <- svm(price~., data=train, kernel="linear", cost=10, scale=TRUE)
summary(svm1)

```
###Evaluate the linear svm (SVM1)
Now we have a model, we can predict the value of the new data set, we got a correlation of 95%, which is the same as the linear regression, the svm1's mse has increased which is not good.

```{r}

pred_svm1 <- predict(svm1, newdata=test)
cor_svm1 <- cor(pred_svm1, test$price)
mse_svm1 <- mean((pred_svm1 - test$price)^2)
rmse_svm1 <- sqrt(mse_svm1)
print(paste('correlation:', cor_svm1))
print(paste('mse:', mse_svm1))
print(paste('rmse:', rmse_svm1))

```
###Linear svm Plot
Plot the Support Vectors

```{r}
plot(pred_svm1)
```



```{r}

x = 1:length(test$price)
plot(x,test$price, pch=18, col="coral4")
lines(x,pred_svm1, lwd="1", col="cadetblue")

```



###Linear svm Tuning
The cost parameter determines how much  slack variables will be allowed.Experiment with various cost values to get the best model.The hyperparameters are tuned on the validation set to not over fit data and not against good principles by letting the algorithm see test data.Larger C have larger margins, smaller C, move the model toward lower bias, higher variance. The summary of tune_svm1 tells us the best cost is 0.1.The next syntax will use the best model value to make predictions on the test data. We got a correlation of 95%, which is the same as the linear regression, the tune_svm1's mse has increased more than svm1 even though it is tuned.

```{r , warning=FALSE}

set.seed(1234)
tune_svm1 <- tune(svm, price~., data=vald, kernel="linear",
            ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm1)
pred <- predict(tune_svm1$best.model, newdata=test)
cor_svm1_tune <- cor(pred, test$price)
mse_svm1_tune <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm1_tune))
print(paste('mse:', mse_svm1_tune))


```

###Polynomial SVM model
Let us build an SVM2 model on the train set using cost=10 and kernel="Polynomial".

```{r}

svm2 <- svm(price~., data=train, kernel="polynomial", cost=10, scale=TRUE)
summary(svm2)

```

###Evaluate the polynomial svm (SVM2)
Let us predict the value of the new data set with the model,SVM2. We got a correlation of 98%, greater than the linear regression, svm1, and tune_svm1, the svm1's mse value has has also lowered by half, which is promising.

```{r}

pred_svm2 <- predict(svm2, newdata=test)
cor_svm2 <- cor(pred_svm2, test$price)
mse_svm2 <- mean((pred_svm2 - test$price)^2)
rmse_svm2 <- sqrt(mse_svm2)
print(paste('correlation:', cor_svm2))
print(paste('mse:', mse_svm2))
print(paste('rmse:', rmse_svm2))

```

###Polynomial svm Plot
Plot the Support Vectors

```{r}
plot(pred_svm2)
```


```{r}

x = 1:length(test$price)
plot(x,test$price, pch=18, col="coral4")
lines(x,pred_svm2, lwd="1", col="blue")

```


###Polynomial svm Tuning
The cost parameter determines how much  slack variables will be allowed.Experiment with various cost values to get the best model.The hyperparameters are tuned on the validation set to not over fit data and not against good principles by letting the algorithm see test data.Larger C have larger margins, smaller C, move the model toward lower bias, higher variance. The summary of tune_svm2 tells us the best cost is 100.The next syntax will use the best model value to make predictions on the test data. We got a correlation of 98%, the same as svm2,but the mse has slightly increased.

```{r , warning=FALSE}
set.seed(1234)
tune_svm2 <- tune(svm, price~., data=vald, kernel="polynomial",
            ranges=list(cost=c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune_svm2)
pred <- predict(tune_svm2$best.model, newdata=test)
cor_svm2_tune <- cor(pred, test$price)
mse_svm2_tune <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm2_tune))
print(paste('mse:', mse_svm2_tune))


```

###Radial Kernel SVM model
Let us build SVM3 model on the train set using cost=1 and kernel="radial" 

```{r}
svm3 <- svm(price~., data=train, kernel="radial", cost=100, gamma=0.5, scale=TRUE)
summary(svm3)

```

###Evaluate the Kernel svm (SVM3)
Let us predict the value of the new data set with the model,SVM3. We got a correlation of 96%, which is lower than svm2 and tune_svm2, but higher than the linear regression and svm1, similarly the mse value is bigger than  mse of svm2 and tune_svm2, but lower than mse of linear regression and svm1.

```{r}

pred_svm3 <- predict(svm3, newdata=test)
cor_svm3 <- cor(pred_svm3, test$price)
mse_svm3 <- mean((pred_svm3 - test$price)^2)
rmse_svm3 <- sqrt(mse_svm3)
print(paste('correlation:', cor_svm3))
print(paste('mse:', mse_svm3))
print(paste('rmse:', rmse_svm3))


```

###SVM3 Classification Plot
Plot the Support Vectors

```{r}
plot(pred_svm3)
```


```{r}
x = 1:length(test$price)
plot(x,test$price, pch=18, col="coral4")
lines(x,pred_svm3, lwd="1", col="cadetblue")
```


###SVM3 Tuning
The gamma hyperparameter is tuned on validation data,larger gamma can over fit and move the model toward high variance, and lower gamma can under fit, leading the model with high bias.The summary of tune_svm3 tells us the best cost is 10 and gamma 0.5.The next syntax will use the best model value to make predictions on the test data. We got a correlation of 96%, which is the same as svm3, the mse is high but not higher than svm1.

```{r, warning=FALSE}

set.seed(1234)
tune_svm3 <- tune(svm, price~., data=vald, kernel="radial",
                 ranges=list(cost=c(0.1,1,10,100,1000),
                             gamma=c(0.5,1,2,3,4)))
summary(tune_svm3)
pred <- predict(tune_svm3$best.model, newdata=test)
cor_svm3_tune <- cor(pred, test$price)
mse_svm3_tune <- mean((pred - test$price)^2)
print(paste('correlation:', cor_svm3_tune))
print(paste('mse:', mse_svm3_tune))

```
###Which model is the best?
Out of all the models, the best model is svm2,Polynomial SVM model with out tuning, it has 98% correlation, and the mse value is smaller almost by half compared to the mse value of all models built in this svm regression notebook.
